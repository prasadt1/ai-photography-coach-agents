"""
KnowledgeAgent: LLM-powered coaching agent using Gemini 1.5 Flash.

Responsibilities:
1. Generate personalized coaching responses based on user queries
2. Incorporate vision analysis results (issues, EXIF data)
3. Maintain conversation context across multiple turns
4. Retrieve relevant photography principles from knowledge base
5. Create actionable practice exercises

Key Innovation: This is NOT a template-based system. Every response is dynamically
generated by Gemini 1.5 Flash, enabling nuanced, context-aware coaching that adapts
to user's specific situation and conversation history.

Model Choice: Gemini 1.5 Flash chosen for:
- Fast response times (<2s typical latency)
- Strong instruction following for coaching scenarios
- Good balance of quality and cost for production deployment
"""

from dataclasses import dataclass
from typing import List, Optional, Dict, Any

import google.generativeai as genai
from agents_capstone.tools.knowledge_base import simple_retrieve, Principle

# Lazy import for Hybrid RAG (optional dependency)
_agentic_rag = None

def _get_agentic_rag():
    """Lazy-load AgenticRAG to avoid startup overhead"""
    global _agentic_rag
    if _agentic_rag is None:
        try:
            from agents_capstone.tools.agentic_rag import AgenticRAG
            _agentic_rag = AgenticRAG(enable_faiss=True)
        except Exception as e:
            print(f"⚠️ AgenticRAG not available: {e}")
            _agentic_rag = False  # Disable future attempts
    return _agentic_rag if _agentic_rag is not False else None

@dataclass
class CoachingResponse:
    """Structured coaching output for UI rendering.
    
    This format provides everything the Streamlit UI needs to display
    a complete coaching interaction.
    """
    text: str  # Main coaching response from Gemini LLM
    principles: List[Principle]  # Referenced photography principles
    issues: List[str]  # Issues from vision analysis
    exercise: str  # Practice exercise for skill improvement

class KnowledgeAgent:
    """Turns analysis + question + history into coaching text using LLM.
    
    Architecture: This agent is stateless - all context is passed in through
    the session parameter. The Orchestrator manages state persistence.
    """

    def coach(
        self,
        query: str,
        vision_analysis: Optional[object],
        session: dict,
    ) -> CoachingResponse:
        """Generate coaching response using LLM with conversation context.
        
        Coaching Pipeline:
        1. Extract issues from vision analysis
        2. Build conversation history context (last 3 turns)
        3. Retrieve relevant photography principles from knowledge base
        4. Call Gemini 1.5 Flash with structured prompt
        5. Generate practice exercise based on issues
        
        Args:
            query: User's current question/request
            vision_analysis: Optional VisionAnalysis from VisionAgent
            session: Dict with conversation history and user metadata
            
        Returns:
            CoachingResponse with LLM text, principles, issues, and exercise
        """
        # Step 1: Extract issues detected by VisionAgent
        issues: List[str] = []
        if vision_analysis is not None:
            issues = list(getattr(vision_analysis, "issues", []))

        # Step 2: Build conversation context from history
        # Design: Only last 3 turns included to prevent token overflow
        # Orchestrator handles longer-term context compaction
        history_context = self._build_history_context(session)
        
        # Step 3: Retrieve relevant principles from knowledge base
        # Simple RAG: Combines user query + detected issues for semantic search
        retrieval_query = query + " " + " ".join(issues)
        principles = simple_retrieve(retrieval_query)
        
        # Step 4: Get dynamic response from Gemini 1.5 Flash
        # Key: This is NOT a template - every response is freshly generated
        coaching_text = self._get_llm_coaching(
            query=query,
            issues=issues,
            history=history_context,
            principles=principles,
        )

        # Step 5: Generate actionable practice exercise
        exercise = self._generate_exercise(issues)

        return CoachingResponse(
            text=coaching_text,
            principles=principles,
            issues=issues,
            exercise=exercise,
        )

    def _build_history_context(self, session: Dict[str, Any]) -> str:
        """Build context string from conversation history.
        
        Context Window Strategy:
        - Only last 3 turns included (prevents token overflow)
        - Each turn includes the user's query
        - Orchestrator handles longer-term summarization via compact_context()
        
        Args:
            session: Dict containing conversation history
            
        Returns:
            Formatted string of recent conversation for LLM prompt
        """
        history = session.get("history", [])
        if not history:
            return "This is the start of the conversation."
        
        context_lines = []
        for i, entry in enumerate(history[-3:]):  # Last 3 turns for context
            query = entry.get("query", "")
            if query:
                context_lines.append(f"- Previous question {i+1}: {query}")
        
        return "\n".join(context_lines) if context_lines else "This is the start of the conversation."

    def _get_llm_coaching(
        self,
        query: str,
        issues: List[str],
        history: str,
        principles: List[Principle],
    ) -> str:
        """Get coaching response from Gemini LLM with Hybrid RAG grounding.
        
        Prompt Engineering Strategy:
        - Structured sections (Question, Issues, Principles, History)
        - Clear coaching guidelines (specific, actionable, concise)
        - Persona instruction ("friendly photography coach")
        - Context limits (3-4 sentences) to maintain focus
        
        Hybrid RAG Enhancement (Phase 1.5):
        - Gemini generates creative, photo-specific coaching
        - AgenticRAG adds grounded citations from curated + PDF knowledge
        - CASCADE: Curated books (high quality) → FAISS PDFs (broad coverage)
        
        Model: gemini-1.5-flash chosen for speed and quality balance
        Fallback: Template-based response if API fails (prevents UI errors)
        
        Args:
            query: User's current question
            issues: List of detected photo issues
            history: Formatted conversation context
            principles: Retrieved photography principles
            
        Returns:
            LLM-generated coaching text with grounded citations (if RAG available)
        """
        try:
            # Build principles context from knowledge base retrieval
            principles_text = "\n".join([
                f"- {p.topic}: {p.text}" for p in principles[:3]
            ]) if principles else "No specific principles found."

            # Structured prompt with clear sections and coaching guidelines
            prompt = f"""You are an expert photography coach providing personalized guidance.

User's Current Question: {query}

Detected Issues in Photo:
{chr(10).join(f"- {issue}" for issue in issues) if issues else "- No issues detected"}

Photography Principles to Consider:
{principles_text}

Conversation Context (Previous Questions):
{history}

Provide helpful, specific photography coaching that:
1. Directly addresses the user's current question
2. References any detected issues in the photo
3. Gives actionable advice they can apply immediately
4. Builds on previous conversation context if applicable
5. Stays focused and concise (3-4 sentences)

Respond as a friendly photography coach, not as a template."""

            # Call Gemini API (using stable flash model)
            # Note: API key configured globally via genai.configure()
            model = genai.GenerativeModel("gemini-2.5-flash")
            response = model.generate_content(prompt)
            creative_response = response.text
            
            # HYBRID RAG: Add grounded citations to creative response
            # This combines Gemini's creativity with authoritative sources
            agentic_rag = _get_agentic_rag()
            if agentic_rag:
                # Ground the response with citations (CASCADE: curated → FAISS)
                grounded_response = agentic_rag.ground_response(
                    creative_response=creative_response,
                    user_query=query,
                    max_citations=2
                )
                return grounded_response
            else:
                # RAG not available, return Gemini's response as-is
                return creative_response
                
        except Exception as e:
            # Fallback if LLM fails (API errors, rate limits, network issues)
            # Log the error for debugging
            import sys
            print(f"DEBUG KnowledgeAgent LLM error: {type(e).__name__}: {str(e)[:200]}", file=sys.stderr)
            return self._generate_fallback_response(query, issues)

    def _generate_fallback_response(self, query: str, issues: List[str]) -> str:
        """Generate fallback response if LLM is unavailable.
        
        Fallback Strategy: Simple keyword matching for common photography topics
        This ensures the UI always displays something useful even during API failures.
        
        Production Note: In deployed system, fallback responses logged for monitoring
        
        Args:
            query: User's question
            issues: Detected photo issues
            
        Returns:
            Template-based coaching response
        """
        response = f"Based on your question about {query}:\n\n"
        
        # Pattern matching for common photography topics
        if "composition" in query.lower():
            response += "For composition: "
            if "subject_centered" in issues:
                response += "Consider moving the main subject to the rule of thirds. "
            response += "Check the horizon line and use leading lines to guide the viewer."
        elif "lighting" in query.lower():
            response += "Lighting is key to great photos. Look for directional light, avoid harsh shadows, and consider the time of day."
        elif "iso" in query.lower() or "settings" in query.lower():
            response += "Adjust ISO based on available light - lower ISO for bright conditions, higher for low light. Balance with aperture and shutter speed."
        elif "about" in query.lower() or "subject" in query.lower():
            response += "Your photo shows interesting elements. Focus on what draws your eye most, and frame to emphasize that."
        else:
            response += "Great question about photography. Keep practicing and experimenting with different perspectives and settings."
        
        return response

    def _generate_exercise(self, issues: List[str]) -> str:
        """Generate a practice exercise based on detected issues.
        
        Uses Gemini to create personalized, issue-specific exercises
        instead of generic templates.
        
        Args:
            issues: List of detected photo issues
            
        Returns:
            Actionable practice exercise string
        """
        if not issues:
            return "Exercise: Practice the rule of thirds - take 10 photos placing subjects at different grid intersections."
        
        try:
            # Use Gemini to generate personalized exercise
            prompt = f"""Based on these photography issues detected in a photo: {', '.join(issues)}

Generate ONE specific, actionable 30-minute practice exercise to improve these skills.

Requirements:
- Focus on the MOST critical issue
- Be specific (e.g., "take 10 photos" not "practice composition")
- Include what to look for or measure
- Keep it under 25 words
- Start with "Exercise: "

Example: "Exercise: Take 15 shots of one subject, changing only your position. Compare how angles affect visual impact."

Your exercise:"""

            model = genai.GenerativeModel("gemini-2.5-flash")
            response = model.generate_content(prompt)
            exercise = response.text.strip()
            
            # Ensure it starts with "Exercise: "
            if not exercise.startswith("Exercise:"):
                exercise = "Exercise: " + exercise
            
            return exercise
            
        except Exception as e:
            # Fallback to issue-specific templates
            if any(term in ' '.join(issues).lower() for term in ['center', 'composition', 'thirds']):
                return "Exercise: Take 10 photos of the same scene, placing subjects at different rule of thirds intersections."
            elif any(term in ' '.join(issues).lower() for term in ['focus', 'depth', 'sharp']):
                return "Exercise: Practice depth of field - take 5 shots with f/2.8, f/5.6, and f/11 of the same scene."
            elif any(term in ' '.join(issues).lower() for term in ['horizon', 'tilt', 'level']):
                return "Exercise: Take 10 photos focusing on keeping horizons perfectly level using grid lines."
            else:
                return "Exercise: Take 10 photos varying one element (angle, distance, or framing) to see what improves composition."
